{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peiyulan/Embeddedsystem/blob/master/Notebooks/2_WorkingWithTextInNLTK_2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdd705aa",
      "metadata": {
        "id": "cdd705aa"
      },
      "source": [
        "# NLTK with Lewis Grassic Gibbon First Editions\n",
        "\n",
        "**Data Source:** [National Library of Scotland Data Foundry](https://data.nls.uk/data/digitised-collections/lewis-grassic-gibbon-first-editions/)\n",
        "\n",
        "**Code Reference:**\n",
        "National Library of Scotland. Exploring Lewis Grassic Gibbon First Editions. National Library of Scotland, 2020. https://doi.org/10.34812/gq6w-6e91\n",
        "\n",
        "**Date:** Feb 23, 2026\n",
        "\n",
        "**Course:** Text Analysis with NLTK (Week 2); Centre for Data, Culture & Society"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3121b01a",
      "metadata": {
        "id": "3121b01a"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "I. [Loading Data](#Loading_Data)\n",
        "\n",
        "II. [Pre-processing](#pre-processing)\n",
        "\n",
        "III. [Data Cleaning](#data_cleaning)\n",
        "\n",
        "IV. [Analysis](#analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "637cdd3f",
      "metadata": {
        "id": "637cdd3f"
      },
      "source": [
        "<a id=\"Loading_Data\"></a>\n",
        "## I. Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c52a5768",
      "metadata": {
        "id": "c52a5768"
      },
      "outputs": [],
      "source": [
        "# To load a CSV file with an inventory of the documents in the corpus\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# To create data visualizations\n",
        "import altair as alt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# To perform text analysis\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.text import Text\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.draw.dispersion import dispersion_plot as displt\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "import re # Regular Expressions (RegEx)\n",
        "import string\n",
        "import random\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e75de13",
      "metadata": {
        "id": "5e75de13"
      },
      "source": [
        "To get a sense of the data we're working with, let's create have a look at the documentation of nltk twitter sample corpus:\n",
        "\n",
        "https://www.nltk.org/howto/corpus.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can follow the instruction of the documentation:"
      ],
      "metadata": {
        "id": "3qWjBzuYImPe"
      },
      "id": "3qWjBzuYImPe"
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_samples.fileids()"
      ],
      "metadata": {
        "id": "TxX0tiCUIlFA"
      },
      "id": "TxX0tiCUIlFA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows that the corpus has been divided into three parts: negative, positive, and a combination of them.\n",
        "\n",
        "As we want to do sentiment analysis, let's store the data into\n",
        "`positive_twts` and `negative_twts`.\n",
        "\n",
        "We can then a look at the size of the dataset."
      ],
      "metadata": {
        "id": "DuCw0UyyJAAL"
      },
      "id": "DuCw0UyyJAAL"
    },
    {
      "cell_type": "code",
      "source": [
        "positive_twts = twitter_samples.strings('positive_tweets.json')\n",
        "negative_twts = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "1PgkV7DHH9FV"
      },
      "id": "1PgkV7DHH9FV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Positive Tweets: ' + str(len(positive_twts)))\n",
        "print('Negative Tweets: ' + str(len(negative_twts)))"
      ],
      "metadata": {
        "id": "xA5RZzJdJtSF"
      },
      "id": "xA5RZzJdJtSF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a quite balanced dataset as each category has 5000 data point.\n",
        "\n",
        "Now let's take a look at the actual tweets in the corpus."
      ],
      "metadata": {
        "id": "2tCSJYvpJ4mT"
      },
      "id": "2tCSJYvpJ4mT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98374056",
      "metadata": {
        "id": "98374056"
      },
      "outputs": [],
      "source": [
        "print(\"positive:\", positive_twts[5])\n",
        "print(\"negative:\",negative_twts[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "QOoNuaaALioO"
      },
      "id": "QOoNuaaALioO"
    },
    {
      "cell_type": "markdown",
      "id": "109d0e4e",
      "metadata": {
        "id": "109d0e4e"
      },
      "source": [
        "<a id=\"pre-processing\"></a>\n",
        "## II. Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42680cb2",
      "metadata": {
        "id": "42680cb2"
      },
      "source": [
        "As we discussed last week, it’s important to clean up the text before starting your analysis. This helps remove unwanted noise and errors, generating to more reliable results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, pre-processing includes the following steps:\n",
        "\n",
        "1. Use regular expressions (regex) to remove punctuation and unwanted characters.\n",
        "2. Normalise the text by converting it to lowercase.\n",
        "3. Perform tokenisation.\n",
        "4. Remove stopwords.\n",
        "5. Apply stemming or lemmatisation."
      ],
      "metadata": {
        "id": "TYCoDHiFNPrZ"
      },
      "id": "TYCoDHiFNPrZ"
    },
    {
      "cell_type": "markdown",
      "id": "instant-latex",
      "metadata": {
        "id": "instant-latex"
      },
      "source": [
        "### Regular Expresssions (RegEx)\n",
        "\n",
        "* **WHAT? Pattern matching strings in Python**\n",
        "* **WHY? To find specific words or phrases, or variations of a particular word or phrase**\n",
        "    * Once found, they can be replaced, so this is useful for cleaning text with digitization errors.  Optical Character Recognition (OCR) and Handwriting Recognition (HWT or HRT) technologies are imperfect, so you will find errors in digitized text corpora (unless of course they've been manually reviewed and corrected).\n",
        "* **HOW? Combinations of special characters with a RegEx compiler**\n",
        "    * In programming, a *compiler* translates code from one programming language to another.  In a sense, RegEx is a language that can sit on top of Python.  RegEx works with Python data types and syntax but it also has its own special characters and methods that plain Python doesn't use.\n",
        "    \n",
        "Resource for practice with and testing Regular Expressions: [Regex101.com](https://regex101.com): also check out [W3Schools](https://www.w3schools.com/python/python_regex.asp) for the cheat sheet it provides!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "complex-questionnaire",
      "metadata": {
        "id": "complex-questionnaire"
      },
      "outputs": [],
      "source": [
        "# # To use Regular Expressions (RegEx)\n",
        "# import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "facial-snowboard",
      "metadata": {
        "id": "facial-snowboard"
      },
      "source": [
        "To remove a substring (a selection of characters in a string), we can use an empty string (either `\"\"` or `''`) as the second input for the `replace()` method."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Jenny and Josh are having lunch.\"\n",
        "new_txt = re.sub(r'Jenny', 'Peter', txt)\n",
        "print(txt)\n",
        "print(new_txt)"
      ],
      "metadata": {
        "id": "pA-vLtbLP8YP"
      },
      "id": "pA-vLtbLP8YP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `[]` A set of characters. ex: \"`[a-m]`\"mean a to m\n",
        "2. `\\w`\tReturns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character)\n",
        "3. `+`\tOne or more occurrences"
      ],
      "metadata": {
        "id": "vEd-L066RwZ_"
      },
      "id": "vEd-L066RwZ_"
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"@BhaktisBanter @PallaviRuhail This one is irresistible :)\"\n",
        "new_txt = re.sub(r'@[\\w]+', '', txt)\n",
        "print(txt)\n",
        "print(new_txt)"
      ],
      "metadata": {
        "id": "OtxcLpkQQ_8e"
      },
      "id": "OtxcLpkQQ_8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try to remove links in the following text:"
      ],
      "metadata": {
        "id": "GTj6fgDTTFAK"
      },
      "id": "GTj6fgDTTFAK"
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"#FlipkartFashionFriday http://t.co/EbZ0L2VENM\"\n",
        "\n",
        "new_txt = re.sub(r'YOUR CODE HERE', '', txt)\n",
        "print(txt)\n",
        "print(new_txt)"
      ],
      "metadata": {
        "id": "QFyP1m3zTEPI"
      },
      "id": "QFyP1m3zTEPI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning(tweet):\n",
        "    \"\"\"\n",
        "    Preproceeses a Tweet by removing hashes, RTs, @mentions,\n",
        "    links, stopwords and punctuation, tokenizing and stemming\n",
        "    the words.\n",
        "\n",
        "    Accepts:\n",
        "        tweet {str} -- tweet string\n",
        "\n",
        "    Returns:\n",
        "        {str}\n",
        "    \"\"\"\n",
        "    ## Your code hear\n",
        "    pass\n",
        "    return clean_twt"
      ],
      "metadata": {
        "id": "nuXU-JUeN-fv"
      },
      "id": "nuXU-JUeN-fv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_positive_twts = [cleaning(tweet) for tweet in positive_twts]\n",
        "clean_negative_twts = [cleaning(tweet) for tweet in negative_twts]\n",
        "print(clean_positive_twts[5])"
      ],
      "metadata": {
        "id": "J_1dY09eU80m"
      },
      "id": "J_1dY09eU80m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "SNJUYNj9WpLr"
      },
      "id": "SNJUYNj9WpLr"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TweetTokenizer()\n",
        "twt_tokens = tokenizer.tokenize(clean_positive_twts[5])\n",
        "print(twt_tokens)"
      ],
      "metadata": {
        "id": "fKF_TX2DWrfW"
      },
      "id": "fKF_TX2DWrfW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tokens = [tokenizer.tokenize(tweet) for tweet in clean_positive_twts]\n",
        "print(pos_tokens[5])"
      ],
      "metadata": {
        "id": "kgWASgRyXZL2"
      },
      "id": "kgWASgRyXZL2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lowercasing"
      ],
      "metadata": {
        "id": "IquVVCROTm0H"
      },
      "id": "IquVVCROTm0H"
    },
    {
      "cell_type": "markdown",
      "id": "5cd443fb",
      "metadata": {
        "id": "5cd443fb"
      },
      "source": [
        "Let's casefold to normalize so capitalized and lowercased versions of words are considered the same word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b7cd02",
      "metadata": {
        "id": "75b7cd02"
      },
      "outputs": [],
      "source": [
        "def lowercase_all(words):\n",
        "    # your code here\n",
        "    pass\n",
        "\n",
        "\n",
        "txt = [\"SAMPLE\", \"Text\"]\n",
        "new_text = lowercase_all(txt)\n",
        "print(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tokens = [lowercase_all(token) for token in pos_tokens]\n",
        "neg_tokens = [lowercase_all(token) for token in neg_tokens]"
      ],
      "metadata": {
        "id": "ZDFYpa0eXy3k"
      },
      "id": "ZDFYpa0eXy3k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Stopwords"
      ],
      "metadata": {
        "id": "QLs_n6zmT7Al"
      },
      "id": "QLs_n6zmT7Al"
    },
    {
      "cell_type": "markdown",
      "id": "75409713",
      "metadata": {
        "id": "75409713"
      },
      "source": [
        "...and exclude stopwords using `stopwords.words(language)`:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tokens_nosd = []\n",
        "stopwords_en = stopwords.words('english')\n",
        "for word in pos_tokens[5]:\n",
        "    if word not in stopwords_en and word not in string.punctuation:\n",
        "        pos_tokens_nosd.append(word)\n"
      ],
      "metadata": {
        "id": "DC4Yo1PjYMoh"
      },
      "id": "DC4Yo1PjYMoh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords_en)"
      ],
      "metadata": {
        "id": "8gBvXLoXYpBN"
      },
      "id": "8gBvXLoXYpBN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_tokens[5])\n",
        "print(pos_tokens_nosd)"
      ],
      "metadata": {
        "id": "SPhQ5_i0YN1H"
      },
      "id": "SPhQ5_i0YN1H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenise_tweet(tweets):\n",
        "    clean_tokens = []\n",
        "    twt_tokens = tokenizer.tokenize(tweets)\n",
        "\n",
        "    # lowercasing\n",
        "    #### Your code here\n",
        "\n",
        "    # is alphabetical\n",
        "    #### Your code here\n",
        "\n",
        "\n",
        "    # remove stopword\n",
        "    #### Your code here\n",
        "\n",
        "    return clean_tokens"
      ],
      "metadata": {
        "id": "c3yEBHAOUHMS"
      },
      "id": "c3yEBHAOUHMS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_twt_token = [tokenise_tweet(tweet) for tweet in clean_positive_twts]\n",
        "neg_twt_token = [tokenise_tweet(tweet) for tweet in clean_negative_twts]"
      ],
      "metadata": {
        "id": "gPzG_-4gY-oQ"
      },
      "id": "gPzG_-4gY-oQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"original:\\n\", positive_twts[5])\n",
        "print(\"\\nafter regex:\\n\", clean_positive_twts[5])\n",
        "print(\"\\nnormalised, stemmed, stopwords removed:\\n\",pos_twt[5])"
      ],
      "metadata": {
        "id": "WS3FjcaXaOv4"
      },
      "id": "WS3FjcaXaOv4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9e1cc781",
      "metadata": {
        "id": "9e1cc781"
      },
      "source": [
        "<a id=\"analysis\"></a>\n",
        "## IV. Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def freq_plot(corpus, n):\n",
        "    word = list(itertools.chain.from_iterable(corpus))\n",
        "    fdist = FreqDist(word)\n",
        "    plt.figure(figsize = (20, 8))\n",
        "    plt.rc('font', size=12)\n",
        "    fdist.plot(n, title=f'Frequency Distribution for {n} Most Common Tokens in the Dataset (excluding stop words)')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BykFAjGtdvT_"
      },
      "id": "BykFAjGtdvT_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_plot(pos_twt_token, 100) ## Try increasing or decreasing this number to view more or fewer tokens in the visualization\n",
        "freq_plot(neg_twt_token, 100)"
      ],
      "metadata": {
        "id": "KM2tC5eTjOm-"
      },
      "id": "KM2tC5eTjOm-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = list(itertools.chain.from_iterable(pos_twt_token))\n",
        "text = \" \".join(text)\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SUnObkR0mzqB"
      },
      "id": "SUnObkR0mzqB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b63226db",
      "metadata": {
        "id": "b63226db"
      },
      "source": [
        "Using NLTK’s VADER Sentiment Analyzer (Lexicon-based) as an example perform sentimet analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VADER (Valence Aware Dictionary and sEntiment Reasoner)** is a sentiment analysis tool designed to capture sentiments expressed in **social media**, which makes it particularly effective in analysing tweets.\n",
        "\n",
        "One advantage of VADER is that it does not classify text as positive or negative; it also indicates the intensity of the sentiment. In the output, it provides separate scores for positive, negative, and neutral components.\n",
        "\n",
        "In addition, VADER returns a fourth value, the compound score. This is a normalised metric that aggregates the overall sentiment and ranges from –1 to 1. Based on this score, we determine whether the overall sentiment is positive, negative, or neutral."
      ],
      "metadata": {
        "id": "eO7ar3T2c6eO"
      },
      "id": "eO7ar3T2c6eO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1e653c",
      "metadata": {
        "id": "9b1e653c"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "def get_vader_sentiment(text):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    scores = sia.polarity_scores(text)\n",
        "    compound = scores['compound']\n",
        "    if compound >= 0.05:\n",
        "        return ['positive, socre:',compound]\n",
        "    elif compound <= -0.05:\n",
        "        return ['negative, socre:',compound]\n",
        "    else:\n",
        "        return ['neutral, socre:',compound]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"thank you!\"\n",
        "print(get_vader_sentiment(sample_text))"
      ],
      "metadata": {
        "id": "1QHDfTbgcn23"
      },
      "id": "1QHDfTbgcn23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_vader_sentiment(\"thank\"))\n",
        "print(get_vader_sentiment(\"you!\"))"
      ],
      "metadata": {
        "id": "OjHX4IlvdaMM"
      },
      "id": "OjHX4IlvdaMM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}